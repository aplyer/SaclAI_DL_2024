{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e966a723",
      "metadata": {
        "id": "e966a723"
      },
      "source": [
        "# Introduction aux Physics-informed neural networks (PINNs)\n",
        "\n",
        "Fortement inspiré des travaux de : Ben Moseley, 2022\n",
        "Aurelien Plyer 2022\n",
        "\n",
        "Pour plus d'information sur les PINN's la lecture des papiers suivant est plus que souhaité [here](https://ieeexplore.ieee.org/document/712178) et [here](https://www.sciencedirect.com/science/article/pii/S0021999118307125).\n",
        "\n",
        "\n",
        "## Objectif du Notebook\n",
        "\n",
        "\n",
        "Ce notebook à pour objectif de présenter les PINN's au travers\n",
        "d'un cas applicatif simple d'une masse aoscilente.\n",
        "L'implémentation est fait en Pytorch.\n",
        "\n",
        "Les cas d'études permettrons de mettre en pratique les différents régimes de fonctionnement d'un PINN's"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-inline -qq"
      ],
      "metadata": {
        "id": "dZOKeN-P0fOs"
      },
      "id": "dZOKeN-P0fOs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1bc9f",
      "metadata": {
        "id": "dee1bc9f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(123)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CD2VDHuFc8j0",
      "metadata": {
        "id": "CD2VDHuFc8j0"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from matplotlib_inline import backend_inline\n",
        "backend_inline.set_matplotlib_formats('svg')\n",
        "plt.style.use('dark_background')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f906efe0",
      "metadata": {
        "id": "f906efe0"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == 'cuda':\n",
        "    import torch.backends.cudnn as cudnn\n",
        "    cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a29a35",
      "metadata": {
        "id": "16a29a35"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b64298",
      "metadata": {
        "id": "a4b64298"
      },
      "source": [
        "## Aperçu du problème\n",
        "\n",
        "Nous allons utiliser un PINN pour résoudre des problèmes liés à l'**oscillateur harmonique amorti**.\n",
        "\n",
        "Nous souhaitons modéliser le déplacement de la masse sur un ressort  en fonction du temps.\n",
        "\n",
        "Il s'agit d'un problème de physique canonique, où le déplacement, $u(t)$, de l'oscillateur en fonction du temps peut être décrit par l'équation différentielle suivante :\n",
        "\n",
        "$$\n",
        "m \\dfrac{d^2 u}{d t^2} + \\mu \\dfrac{d u}{d t} + ku = 0~,\n",
        "$$\n",
        "\n",
        "où $m$ est la masse de l'oscillateur, $\\mu$ est le coefficient de frottement et $k$ est la constante du ressort.\n",
        "\n",
        "Nous nous concentrerons sur la résolution du problème dans l'état **sous-amorti**, c'est-à-dire lorsque l'oscillation est lentement amortie par le frottement (comme le montre l'animation ci-dessus).\n",
        "\n",
        "Mathématiquement, cela se produit lorsque :\n",
        "$$\n",
        "\\delta < \\omega_0~,~~~~~\\mathrm{where}~~\\delta = \\dfrac{\\mu}{2m}~,~\\omega_0 = \\sqrt{\\dfrac{k}{m}}~.\n",
        "$$\n",
        "\n",
        "En outre, nous considérons les conditions initiales suivantes du système :\n",
        "\n",
        "$$\n",
        "u(t=0) = 1~~,~~\\dfrac{d u}{d t}(t=0) = 0~.\n",
        "$$\n",
        "\n",
        "Pour ce cas particulier, la solution exacte est connue et donnée par :\n",
        "\n",
        "$$\n",
        "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))~,~~~~~\\mathrm{with}~~\\omega=\\sqrt{\\omega_0^2 - \\delta^2}~.\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159f6977",
      "metadata": {
        "id": "159f6977"
      },
      "source": [
        "## Plan du notebook\n",
        "\n",
        "\n",
        "Il y a **plusieurs tâches** liées à l'oscillateur harmonique pour lesquelles nous utiliserons un PINN :\n",
        "\n",
        "1. Nous allons **régresser** un réseau de neuronnes pour estimer une fonction continue et différentiable à partir des données\n",
        "1.  Nous **simulerons** le système à l'aide d'un réseau neuronal, compte tenu de ses conditions initiales.\n",
        "1. Nous **inverserons** les paramètres sous-jacents du système à l'aide d'un réseau neuronal, compte tenu de certaines observations bruitées du déplacement de l'oscillateur.\n",
        "1. Nous étudierons dans quelle mesure le PINN **s'adapte** à des oscillations de fréquence plus élevée et ce qui peut être fait pour améliorer sa convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d82ee71",
      "metadata": {
        "id": "9d82ee71"
      },
      "source": [
        "## Code global\n",
        "\n",
        "On commence par coder la solution exacte au problème de la masse avec ressort :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac19c5e",
      "metadata": {
        "id": "2ac19c5e"
      },
      "outputs": [],
      "source": [
        "def exact_solution(d, w0, t):\n",
        "    \"Defines the analytical solution to the under-damped harmonic oscillator problem above.\"\n",
        "    assert d < w0\n",
        "    w = np.sqrt(w0**2-d**2)\n",
        "    phi = np.arctan(-d/w)\n",
        "    A = 1/(2*np.cos(phi))\n",
        "    cos = torch.cos(phi+w*t)\n",
        "    exp = torch.exp(-d*t)\n",
        "    u = exp*2*A*cos\n",
        "    return u\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On définit ici un simple réseau MLP à x couches caché et K réseau par couches :"
      ],
      "metadata": {
        "id": "xF6JPlLLOOtx"
      },
      "id": "xF6JPlLLOOtx"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FCN(nn.Module):\n",
        "    \"Defines a standard fully-connected network in PyTorch\"\n",
        "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
        "        super().__init__()\n",
        "        activation = nn.Tanh\n",
        "        self.fcs = nn.Sequential(*[\n",
        "                        nn.Linear(N_INPUT, N_HIDDEN),\n",
        "                        activation()])\n",
        "        self.fch = nn.Sequential(*[\n",
        "                        nn.Sequential(*[\n",
        "                            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
        "                            activation()]) for _ in range(N_LAYERS-1)])\n",
        "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fcs(x)\n",
        "        x = self.fch(x)\n",
        "        x = self.fce(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_8MPCWU9OO50"
      },
      "id": "_8MPCWU9OO50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "DgVRArrLgYpg",
      "metadata": {
        "id": "DgVRArrLgYpg"
      },
      "source": [
        "# Utilisation d'un réseau de neuronne comme un régresseur sur les données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KcAZboTlif_m",
      "metadata": {
        "id": "KcAZboTlif_m"
      },
      "source": [
        "Dans ce premier test on utilisera le réseau de neuronne simplement pour représenter le jeux de donnée. On est alors dans un cas de simple régression.\n",
        "\n",
        "La littérature appel aussi cela une représentation neural implicite, ce qui donnera en vision par ordinnateur 3D la naissance des Nerf par exemple (Neural Radiance Fields).\n",
        "\n",
        "Le problème se traduit donc comme suit :\n",
        "\n",
        "- Inputs: Des points d'apprentissages $\\{(u_i, t_i)\\}_{i=1..N}$\n",
        "- Outputs: un réseau de neuronne qui approxime la fonction $u(t)$\n",
        "\n",
        "Cette approximation possède deux avantage sur les points d'apprentissage :\n",
        "- elle est continue\n",
        "- elle est dérivable automatiquement de manière exacte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sZzzSYXLikvW",
      "metadata": {
        "id": "sZzzSYXLikvW"
      },
      "source": [
        "\n",
        "## Boucle d'apprentissage sur les points sans bruit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MxbLM_3VgcmB",
      "metadata": {
        "id": "MxbLM_3VgcmB"
      },
      "outputs": [],
      "source": [
        "# Number of trainning points\n",
        "N_train = 30\n",
        "N_total = 300\n",
        "factor_train = 0.5\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "# train the PINN\n",
        "d, w0 = 2, 20\n",
        "mu, k = 2*d, w0**2\n",
        "\n",
        "t_test = torch.linspace(0,1,300).view(-1,1)\n",
        "u_exact = exact_solution(d, w0, t_test)\n",
        "\n",
        "# we cheat here to take only the first half of data\n",
        "idx = np.random.choice(int(factor_train*N_total), N_train, replace=False)\n",
        "\n",
        "t_train = t_test[idx]\n",
        "u_train = u_exact[idx]\n",
        "\n",
        "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
        "fig = None\n",
        "\n",
        "for i in range(15001):\n",
        "    optimiser.zero_grad()\n",
        "    u_pred = pinn(t_train.to(device))\n",
        "    loss = torch.mean((u_pred-u_train.to(device))**2)# use mean squared error\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    if i % 500 == 0:\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(8,3))\n",
        "        fig.clf()\n",
        "        plt.scatter(t_train,\n",
        "                    u_train, s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"NN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i}\")\n",
        "        plt.legend()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe ici que même si le NN à extrêmement bien estimé la fonction sur la zone de nos observation, dès lors qu'on s'en éloigne l'estimation ne ressemble à rien de physique.\n",
        "\n",
        "C'est un point important à avoir en tête lors de l'apprentissage de réseaux de neuronnes. En général on est trés bon pour faire de l'interpolation de la variété des données d'apprentissage, par contre dès lors qu'on s'écarte de cette variété de départ l'extrapolation se passe trés mal.\n",
        "\n",
        "Cela à donnée lieu à de nombreux domaine d'étude comme le *transfert learning* et le *continuous learning* qui vont chercher à repousser ces limites."
      ],
      "metadata": {
        "id": "7S36M5H2QR4R"
      },
      "id": "7S36M5H2QR4R"
    },
    {
      "cell_type": "markdown",
      "id": "7dzMpu5AjuSh",
      "metadata": {
        "id": "7dzMpu5AjuSh"
      },
      "source": [
        "## cas de données bruités\n",
        "\n",
        "Bien évidement apprendre sur un jeux de données sans bruit c'est tricher. Regardons ce qu'il se passe si on ajoute un peut de bruit sur les données. Hésitez pas a jouer avec le niveau de bruit (noise_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W59QhF8qjyia",
      "metadata": {
        "id": "W59QhF8qjyia"
      },
      "outputs": [],
      "source": [
        "# Number of trainning points\n",
        "N_train = 30\n",
        "N_total = 300\n",
        "noise_level = 0.04\n",
        "factor_train = 0.8\n",
        "d, w0 = 2, 20\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "mu, k = 2*d, w0**2\n",
        "\n",
        "t_test = torch.linspace(0,1,N_total).view(-1,1)\n",
        "u_exact = exact_solution(d, w0, t_test)\n",
        "\n",
        "# we cheat here to take only the first half of data\n",
        "idx = np.random.choice(int(factor_train*N_total), N_train, replace=False)\n",
        "\n",
        "t_train = t_test[idx]\n",
        "u_train = u_exact[idx]+ noise_level*torch.randn_like(u_exact[idx])\n",
        "\n",
        "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
        "fig = None\n",
        "\n",
        "for i in range(15001):\n",
        "    optimiser.zero_grad()\n",
        "    u_pred = pinn(t_train.to(device))\n",
        "    loss = torch.mean((u_pred-u_train.to(device))**2)# use mean squared error\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    if i % 500 == 0:\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(8,3))\n",
        "        fig.clf()\n",
        "        plt.scatter(t_train,\n",
        "                    u_train, s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"NN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i}\")\n",
        "        plt.legend()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On remarque qu'au fure et a mesure des itérations le model overfit les données.\n",
        "\n",
        "**Question :** explorer les hyper-paramètres afin de voir si vous ne pouvez pas limiter l'overfitting."
      ],
      "metadata": {
        "id": "SpATz1Ni1jnf"
      },
      "id": "SpATz1Ni1jnf"
    },
    {
      "cell_type": "markdown",
      "id": "fOcqelzhenqQ",
      "metadata": {
        "id": "fOcqelzhenqQ"
      },
      "source": [
        "# Simulation de phénomène avec un PINN's\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4kwRqz6PiAhC",
      "metadata": {
        "id": "4kwRqz6PiAhC"
      },
      "source": [
        "#### Tâche\n",
        "\n",
        "La première tâche consiste à utiliser un PINN pour **simuler** le système.\n",
        "\n",
        "Plus précisément, nos entrées et sorties sont les suivantes :\n",
        "\n",
        "- Entrées : équation différentielle sous-jacente et conditions initiales du système\n",
        "- Sorties : estimation de la solution, $u(t)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qvnQVmi6iCvD",
      "metadata": {
        "id": "qvnQVmi6iCvD"
      },
      "source": [
        "#### Approche\n",
        "\n",
        "Le PINN est entraîné à approximer directement la solution de l'équation différentielle, c'est-à-dire\n",
        "\n",
        "$$\n",
        "u_{\\mathrm{PINN}}(t;\\theta) \\approx u(t)~,\n",
        "$$\n",
        "\n",
        "où $\\theta$ sont les paramètres libres du PINN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eA43eoVZiGBG",
      "metadata": {
        "id": "eA43eoVZiGBG"
      },
      "source": [
        "#### Fonction de perte\n",
        "\n",
        "Pour simuler le système, le PINN est entraîné avec la fonction de perte suivante :\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta)=  \\mathcal{L}_0(\\theta) + \\lambda_1 \\mathcal{L}_1(\\theta) + \\lambda_2\\mathcal{L}_2(\\theta)\n",
        "$$\n",
        "\n",
        "où :  \n",
        "\n",
        "$$\n",
        "\\mathcal{L}_0(\\theta)= (u_{\\mathrm{PINN}}(t=0;\\theta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_1(\\theta) = \\left(\\frac{d\\,u_{\\mathrm{PINN}}}{dt}(t=0;\\theta) - 0\\right)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_2(\\theta) =  \\frac{1}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] u_{\\mathrm{PINN}}(t_{i};\\theta)  \\right)^2\n",
        "$$\n",
        "\n",
        "\n",
        "Pour cette tâche, nous utilisons $\\delta=2$, $\\omega_0=20$, et nous essayons d'apprendre la solution sur le domaine $\\in [0,1]$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54deb812",
      "metadata": {
        "id": "54deb812"
      },
      "source": [
        "#### Notes\n",
        "\n",
        "Les deux premiers termes de la fonction de perte $\\mathcal{L}_i(\\theta)$, $i = \\{0,1\\}$  représentent la **condition de bords**, et servent à garantir que la solution apprise par le PINN correspond aux conditions initiales du système, à savoir $u(t=0)=1$ et $u'(t=0)=0$.\n",
        "\n",
        "Le terme $\\mathcal{L}_2$ est appelé **perte physique** et vise à garantir que la solution PINN obéit à l'équation différentielle sous-jacente à un ensemble de points d'apprentissage $\\{t_i\\}$ *(collocation points)* échantillonnés sur l'ensemble du domaine.\n",
        "\n",
        "Les hyperparamètres, $\\lambda_1$ et $\\lambda_2$, sont utilisés pour équilibrer les termes de la fonction de perte, afin de garantir la stabilité pendant l'apprentissage.\n",
        "\n",
        "L'autodifférenciation (`torch.autograd`) est utilisée pour calculer les gradients de la PINN par rapport à son entrée requise pour évaluer la fonction de perte. C'est très puissant !\n",
        "\n",
        "Pour plus de détails sur `torch.autograd`, consultez le tutoriel [this](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#a-gentle-introduction-to-torch-autograd)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SsgqEpIbes9G",
      "metadata": {
        "id": "SsgqEpIbes9G"
      },
      "source": [
        "### Trainning loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6f9321b",
      "metadata": {
        "id": "e6f9321b",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "number_of_physics_point = 30\n",
        "lambda1, lambda2 = 1e-1, 1e-4\n",
        "n_iter = 20001\n",
        "d, w0 = 2, 20\n",
        "N_tot = 300\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "# define boundary points, for the boundary loss\n",
        "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True)\n",
        "# define training points over the entire domain, for the physics loss\n",
        "t_physics = torch.linspace(0,1,number_of_physics_point).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# train the PINN\n",
        "mu, k = 2*d, w0**2\n",
        "t_test = torch.linspace(0,1,N_tot).view(-1,1)\n",
        "u_exact = exact_solution(d, w0, t_test)\n",
        "\n",
        "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
        "fig = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    optimiser.zero_grad()\n",
        "    t_b = t_boundary.to(device)\n",
        "    # compute boundary loss\n",
        "    u = pinn(t_b)\n",
        "    loss1 = (torch.squeeze(u) - 1)**2\n",
        "    dudt = torch.autograd.grad(u, t_b, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    loss2 = (torch.squeeze(dudt) - 0)**2\n",
        "\n",
        "    # compute physics loss\n",
        "    t_p = t_physics.to(device)\n",
        "    u = pinn(t_p)\n",
        "    dudt = torch.autograd.grad(u, t_p, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    d2udt2 = torch.autograd.grad(dudt, t_p, torch.ones_like(dudt).to(device), create_graph=True)[0]\n",
        "    loss3 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
        "\n",
        "    # backpropagate joint loss, take optimiser step\n",
        "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # plot the result as training progresses\n",
        "    if i % 500 == 0:\n",
        "        #print(u.abs().mean().item(), dudt.abs().mean().item(), d2udt2.abs().mean().item())\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(8,3))\n",
        "        fig.clf()\n",
        "        plt.scatter(t_physics.detach()[:,0],\n",
        "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
        "        plt.scatter(t_boundary.detach()[:,0],\n",
        "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i}\")\n",
        "        plt.legend()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression physique avec du bruit\n",
        "\n",
        "Cette fois-ci combinons la regression de donnée avec une fonction de perte physique pour apprendre une représentation neural contraint par la physique.\n"
      ],
      "metadata": {
        "id": "YMmS_467SM69"
      },
      "id": "YMmS_467SM69"
    },
    {
      "cell_type": "code",
      "source": [
        "lambda1 = 1e4\n",
        "n_iter = 20001\n",
        "d, w0 = 2, 20\n",
        "lambda1 = 1e4\n",
        "n_iter = 20001\n",
        "d, w0 = 2, 20\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "\n",
        "# define training points over the entire domain, for the physics loss\n",
        "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# train the PINN\n",
        "_, k = 2*d, w0**2\n",
        "\n",
        "\n",
        "# add mu to the optimiser\n",
        "optimiser = torch.optim.Adam(list(pinn.parameters()),lr=1e-3)\n",
        "for i in range(n_iter):\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # compute physics loss\n",
        "    t_p = t_physics.to(device)\n",
        "    u = pinn(t_p)\n",
        "    dudt = torch.autograd.grad(u, t_p, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    d2udt2 = torch.autograd.grad(dudt, t_p, torch.ones_like(dudt).to(device), create_graph=True)[0]\n",
        "    loss1 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
        "\n",
        "    # compute data loss\n",
        "\n",
        "    u = pinn(t_obs.to(device))\n",
        "    loss2 = torch.mean((u - u_obs.to(device))**2)\n",
        "\n",
        "    # backpropagate joint loss, take optimiser step\n",
        "    loss = loss1 + lambda1*loss2\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "\n",
        "    # plot the result as training progresses\n",
        "    if i % 500 == 0:\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(11,3))\n",
        "        fig.clf()\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        plt.scatter(t_obs[:,0], u_obs[:,0], label=\"Noisy observations\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i}\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)\n"
      ],
      "metadata": {
        "id": "LXKuTnMISMtF"
      },
      "id": "LXKuTnMISMtF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "LkrbwV6zkxTx",
      "metadata": {
        "id": "LkrbwV6zkxTx"
      },
      "source": [
        "## inversion de paramètres par PINN's"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MlYTxRNlkzv5",
      "metadata": {
        "id": "MlYTxRNlkzv5"
      },
      "source": [
        "#### Tâche\n",
        "\n",
        "La deuxième tâche consiste à utiliser un PINN pour **inverser** les paramètres sous-jacents.\n",
        "\n",
        "Plus précisément, nos entrées et sorties sont les suivantes\n",
        "\n",
        "- Entrées : observations bruitées du déplacement de l'oscillateur\n",
        "- Sorties : estimation de $\\mu$, le coefficient de friction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dv6amvgGk25V",
      "metadata": {
        "id": "dv6amvgGk25V"
      },
      "source": [
        "#### Approche\n",
        "\n",
        "Comme ci-dessus, le PINN est entraîné à approximer directement la solution de l'équation différentielle, c'est-à-dire\n",
        "\n",
        "$$\n",
        "u_{\\mathrm{PINN}}(t;\\theta) \\approx u(t)~,\n",
        "$$\n",
        "\n",
        "où $\\theta$ sont les paramètres libres de la PINN.\n",
        "\n",
        "L'idée clé ici est de traiter $\\mu$ comme un **paramètre apprenable** lors de l'apprentissage de la PINN - de sorte que nous simulons la solution et inversons pour ce paramètre."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TNYm8jgtk6gJ",
      "metadata": {
        "id": "TNYm8jgtk6gJ"
      },
      "source": [
        "#### Loss function\n",
        "\n",
        "Le PINN est formé avec une fonction de perte légèrement différente\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\mu)= \\frac{1}{N} \\sum^{N}_{i} \\left( \\left[ m\\frac{d^2}{dt^2} + \\mu \\frac{d}{dt} + k \\right] u_{\\mathrm{PINN}}(t_{i};\\theta)  \\right)^2 + \\frac{\\lambda}{M} \\sum^{M}_{j} \\left( u_{\\mathrm{PINN}}(t_{j};\\theta) - u_{\\mathrm{obs}}(t_{j}) \\right)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b680afed",
      "metadata": {
        "id": "b680afed"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Notes\n",
        "\n",
        "La fonction de perte comporte deux termes. Le premier est la **perte de physique**, formée de la même manière que ci-dessus, qui garantit que la solution apprise par la PINN est cohérente avec la physique connue.\n",
        "\n",
        "Le second terme est appelé **perte de données**, et assure que la solution apprise par le PINN correspond aux observations (potentiellement bruitées) de la solution qui sont disponibles.\n",
        "\n",
        "Nous avons supprimé les termes de perte aux limites, car nous ne les connaissons pas (nous ne disposons que des mesures observées du système).\n",
        "\n",
        "Dans cette configuration, les paramètres PINN $\\theta$ et $\\mu$ sont **conjointement** appris pendant l'optimisation.\n",
        "\n",
        "Là encore, l'autodifférenciation est notre amie et nous permettra de définir facilement ce problème !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5871be2b",
      "metadata": {
        "id": "5871be2b"
      },
      "outputs": [],
      "source": [
        "# first, create some noisy observational data\n",
        "\n",
        "d, w0 = 2, 20\n",
        "print(f\"True value of mu: {2*d}\")\n",
        "t_obs = torch.rand(40).view(-1,1)*0.5\n",
        "u_obs = exact_solution(d, w0, t_obs) + 0.04*torch.randn_like(t_obs)\n",
        "\n",
        "plt.figure(figsize = (8,4))\n",
        "plt.title(\"Noisy observational data\")\n",
        "plt.scatter(t_obs[:,0], u_obs[:,0])\n",
        "t_test, u_exact = torch.linspace(0,1,300).view(-1,1), exact_solution(d, w0, t_test)\n",
        "plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "_ = plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2wGROpMYmWk4",
      "metadata": {
        "id": "2wGROpMYmWk4"
      },
      "source": [
        "### Trainning loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df8a1a0",
      "metadata": {
        "id": "7df8a1a0"
      },
      "outputs": [],
      "source": [
        "lambda1 = 1e4\n",
        "n_iter = 20001\n",
        "d, w0 = 2, 20\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "\n",
        "# define training points over the entire domain, for the physics loss\n",
        "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# train the PINN\n",
        "_, k = 2*d, w0**2\n",
        "\n",
        "# treat mu as a learnable parameter\n",
        "mu = torch.nn.Parameter(1+torch.zeros(1, requires_grad=True, device = device))\n",
        "mus = []\n",
        "\n",
        "# add mu to the optimiser\n",
        "optimiser = torch.optim.Adam(list(pinn.parameters())+[mu],lr=1e-3)\n",
        "for i in range(n_iter):\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # compute physics loss\n",
        "    t_p = t_physics.to(device)\n",
        "    u = pinn(t_p)\n",
        "    dudt = torch.autograd.grad(u, t_p, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    d2udt2 = torch.autograd.grad(dudt, t_p, torch.ones_like(dudt).to(device), create_graph=True)[0]\n",
        "    loss1 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
        "\n",
        "    # compute data loss\n",
        "\n",
        "    u = pinn(t_obs.to(device))\n",
        "    loss2 = torch.mean((u - u_obs.to(device))**2)\n",
        "\n",
        "    # backpropagate joint loss, take optimiser step\n",
        "    loss = loss1 + lambda1*loss2\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # record mu value\n",
        "    mus.append(mu.item())\n",
        "\n",
        "    # plot the result as training progresses\n",
        "    if i % 500 == 0:\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(11,3))\n",
        "        fig.clf()\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        plt.scatter(t_obs[:,0], u_obs[:,0], label=\"Noisy observations\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i} $\\mu$ = %0.3f\"%mu.item())\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RfcQGlQnmj6J",
      "metadata": {
        "id": "RfcQGlQnmj6J"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.title(\"$\\mu$\")\n",
        "plt.plot(mus, label=\"PINN estimate\")\n",
        "plt.hlines(2*d, 0, len(mus), label=\"True value\", color=\"tab:green\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2JSSqfEcodRt",
      "metadata": {
        "id": "2JSSqfEcodRt"
      },
      "source": [
        "# Lien entre fréquence et échantillonage\n",
        "\n",
        "Il y a un lien entre la fréquence du phénomène que le réseau de neuronne peut reconstruire et l'écartement des points de colocations, c'est ce que nous allons étudier ici."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7S7Ebj-6pWyj",
      "metadata": {
        "id": "7S7Ebj-6pWyj"
      },
      "source": [
        "#### Tâche\n",
        "\n",
        "La tâche finale consiste à étudier la capacité du PINN à **s'adapter** à des oscillations de plus haute fréquence et ce qui peut être fait pour améliorer sa convergence.\n",
        "\n",
        "Plus précisément, nous revenons à la simulation de la solution de l'oscillateur harmonique et augmentons sa fréquence, $\\omega_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wuD2gWcKpYSk",
      "metadata": {
        "id": "wuD2gWcKpYSk"
      },
      "source": [
        "#### Standard formulation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chR-sLObqaz1",
      "metadata": {
        "id": "chR-sLObqaz1"
      },
      "outputs": [],
      "source": [
        "number_of_physics_point = 1000\n",
        "lambda1, lambda2 = 1e-1, 1e-4\n",
        "n_iter = 20001\n",
        "d, w0 = 2, 80\n",
        "N_tot = 300\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3).to(device)\n",
        "# define boundary points, for the boundary loss\n",
        "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True)\n",
        "# define training points over the entire domain, for the physics loss\n",
        "t_physics = torch.linspace(0,1,number_of_physics_point).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# train the PINN\n",
        "mu, k = 2*d, w0**2\n",
        "t_test = torch.linspace(0,1,N_tot).view(-1,1)\n",
        "u_exact = exact_solution(d, w0, t_test)\n",
        "\n",
        "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
        "fig = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # compute boundary loss\n",
        "    t_b= t_boundary.to(device)\n",
        "    u = pinn(t_b)\n",
        "    loss1 = (torch.squeeze(u) - 1)**2\n",
        "    dudt = torch.autograd.grad(u, t_b, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    loss2 = (torch.squeeze(dudt) - 0)**2\n",
        "\n",
        "    # compute physics loss\n",
        "    t_p = t_physics.to(device)\n",
        "    u = pinn(t_p)\n",
        "    dudt = torch.autograd.grad(u, t_p, torch.ones_like(u).to(device), create_graph=True)[0]\n",
        "    d2udt2 = torch.autograd.grad(dudt, t_p, torch.ones_like(dudt).to(device), create_graph=True)[0]\n",
        "    loss3 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
        "\n",
        "    # backpropagate joint loss, take optimiser step\n",
        "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # plot the result as training progresses\n",
        "    if i % 500 == 0:\n",
        "        #print(u.abs().mean().item(), dudt.abs().mean().item(), d2udt2.abs().mean().item())\n",
        "        u = pinn(t_test.to(device)).detach().cpu()\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(8,3))\n",
        "        fig.clf()\n",
        "        plt.scatter(t_physics.detach()[:,0],\n",
        "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
        "        plt.scatter(t_boundary.detach()[:,0],\n",
        "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i}\")\n",
        "        plt.legend()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uABPb5PCqYC0",
      "metadata": {
        "id": "uABPb5PCqYC0"
      },
      "source": [
        "Vous devriez constater que le PINN peine à converger, même si le nombre de points d'entraînement en physique est augmenté.\n",
        "\n",
        "Il s'agit d'un problème plus difficile à résoudre pour le PINN, en partie à cause du **biais spectral** des réseaux neuronaux, ainsi que du fait qu'un plus grand nombre de points d'entraînement est nécessaire.\n",
        "\n",
        "Une autre solution pourrait être d'utiliser des couches de transformation du type des [fourrier features](https://bmild.github.io/fourfeat/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6175ad",
      "metadata": {
        "id": "db6175ad"
      },
      "source": [
        "# Approche : formulation alternative de l'\"ansatz\n",
        "\n",
        "Pour accélérer la convergence, on peut **supposer quelque chose** à propos de la solution.\n",
        "\n",
        "Par exemple, supposons que nous sachions d'après notre intuition physique que la solution est en fait sinusoïdale.\n",
        "\n",
        "Alors, au lieu de faire en sorte que le PINN approxime directement la solution de l'équation différentielle, c'est-à-dire\n",
        "\n",
        "$$\n",
        "u_{\\mathrm{PINN}}(t;\\theta) \\approx u(t)~,\n",
        "$$\n",
        "Nous utilisons plutôt le PINN dans le cadre d'un ansatz mathématique de la solution, c'est-à-dire\n",
        "$$\n",
        "\\hat u(t; \\theta, \\alpha, \\beta) = u_{\\mathrm{PINN}}(t;\\theta)  \\sin (\\alpha t + \\beta) \\approx u(t)~,\n",
        "$$\n",
        "où $\\alpha, \\beta$ sont traités comme des paramètres supplémentaires pouvant être appris.\n",
        "\n",
        "En comparant cet ansatz à la solution exacte\n",
        "$$\n",
        "u(t) = e^{-\\delta t}(2 A \\cos(\\phi + \\omega t))\n",
        "$$\n",
        "Nous constatons que le PINN n'a plus qu'à apprendre la fonction exponentielle, ce qui devrait être un problème beaucoup plus facile.\n",
        "\n",
        "Une fois encore, l'autodifférenciation nous permet de différencier facilement par le biais de cet ansatz pour former le PINN !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b562416d",
      "metadata": {
        "id": "b562416d"
      },
      "outputs": [],
      "source": [
        "n_iter = 20001\n",
        "a_init = 70\n",
        "lambda1, lambda2 = 1e-1, 1e-3\n",
        "d, w0 = 2, 60# note w0 is higher!\n",
        "\n",
        "\n",
        "# define a neural network to train\n",
        "pinn = FCN(1,1,32,3)\n",
        "\n",
        "# define additional a,b learnable parameters in the ansatz\n",
        "a = torch.nn.Parameter(a_init*torch.ones(1, requires_grad=True))\n",
        "b = torch.nn.Parameter(torch.ones(1, requires_grad=True))\n",
        "\n",
        "# define boundary points, for the boundary loss\n",
        "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# define training points over the entire domain, for the physics loss\n",
        "t_physics = torch.linspace(0,1,60).view(-1,1).requires_grad_(True)\n",
        "\n",
        "# train the PINN\n",
        "mu, k = 2*d, w0**2\n",
        "t_test = torch.linspace(0,1,300).view(-1,1)\n",
        "u_exact = exact_solution(d, w0, t_test)\n",
        "optimiser = torch.optim.Adam(list(pinn.parameters())+[a,b],lr=1e-3)\n",
        "fig = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # compute boundary loss\n",
        "    u = pinn(t_boundary)*torch.sin(a*t_boundary+b)\n",
        "    loss1 = (torch.squeeze(u) - 1)**2\n",
        "    dudt = torch.autograd.grad(u, t_boundary, torch.ones_like(u), create_graph=True)[0]\n",
        "    loss2 = (torch.squeeze(dudt) - 0)**2\n",
        "\n",
        "    # compute physics loss\n",
        "    u = pinn(t_physics)*torch.sin(a*t_physics+b)\n",
        "    dudt = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]\n",
        "    d2udt2 = torch.autograd.grad(dudt, t_physics, torch.ones_like(dudt), create_graph=True)[0]\n",
        "    loss3 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
        "\n",
        "    # backpropagate joint loss, take optimiser step\n",
        "\n",
        "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # plot the result as training progresses\n",
        "    if i % 500 == 0:\n",
        "        if fig is None:\n",
        "            fig = plt.figure(figsize=(8,3))\n",
        "        fig.clf()\n",
        "        u = (pinn(t_test)*torch.sin(a*t_test+b)).detach()\n",
        "        plt.scatter(t_physics.detach()[:,0],\n",
        "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
        "        plt.scatter(t_boundary.detach()[:,0],\n",
        "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
        "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
        "        plt.title(f\"Training step {i} loss = %0.3f\"%loss1.item())\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        display.display(fig)\n",
        "        display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd85665f",
      "metadata": {
        "id": "dd85665f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9d82ee71"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}